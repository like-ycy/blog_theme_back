---
title:      " 运维面试题系列--思路分析类问题 "
date:       2021-03-03
banner_img: "https://gitee.com/like-ycy/images/raw/master/blog/header.jpg"
tags: [ 面试 ]
---

# 思路分析类型

### 1、排查java进程cpu占用过高（内存占用高同理）

1、使用top命令查看占用CPU过高的进程，获取到其PID；

2、jstack pid > java.txt  导出CPU占用高进程的线程栈；

3、top -H -p PID 查看对应进程的哪个线程占用CPU过高；

4、在java.txt文件中根据线程id找到对应的线程栈（需要将线程id转换为16进制）；

5、分析负载高的线程栈都是什么业务操作，优化程序。

### 2、cpu负载高的分析

##### 情况1：CPU高、Load高

1. 通过top命令查找占用CPU最高的进程PID；
2. 通过top -Hp PID查找占用CPU最高的线程TID;
3. 对于java程序，使用jstack打印线程堆栈信息（可联系业务进行排查定位）；
4. 通过`printf %x tid`打印出最消耗CPU线程的十六进制；
5. 在堆栈信息中查看该线程的堆栈信息；

##### 情况2：CPU低、Load高

1. 通过top命令查看CPU等待IO时间，即`%wa`；
2. 通过`iostat -d -x -m 1 10`查看磁盘IO情况；(安装命令 `yum install -y sysstat`)
3. 通过`sar -n DEV 1 10`查看网络IO情况；
4. 通过如下命令查找占用IO的程序；

```
ps -e -L h o state,cmd  | awk '{if($1=="R"||$1=="D"){print $0}}' | sort | uniq -c | sort -k 1nr
```

详情查看[Linux系统中负载较高问题排查思路与解决方法](https://www.cnblogs.com/liuyupen/p/13905967.html)

### 3、linux系统负载怎么判断

1、登录服务器，uptime命令查看负载情况；

2、首先查看负载的三个数值，如果15分钟的高，1分钟的低，那可能为正常波动；

3、如果15分钟的低，1分钟的高，使用top命令查看cpu使用情况，然后根据2问题的步骤进行分析。

### 4、linux系统执行命令很卡

1、系统cpu、内存占满，无法执行操作

2、系统磁盘挂载问题

### 5、linux服务器ssh连接不通，怎么排查

1、云服务器首先要排查是否开启安全组

2、ping命令测试、telnet命令测试

3、查看服务器防火墙状态

4、服务器资源被占满，机器出于卡死状态

### 6、k8s pod之间无法通信

- CNI 网络插件配置错误，导致多主机网络不通，比如

  - IP 网段与现有网络冲突
  - 插件使用了底层网络不支持的协议
  - 忘记开启 IP 转发等
    - `sysctl net.ipv4.ip_forward`
    - `sysctl net.bridge.bridge-nf-call-iptables`

- Pod 网络路由丢失，比如

  - kubenet 要求网络中有 podCIDR 到主机 IP 地址的路由，这些路由如果没有正确配置会导致 Pod 网络通信等问题
  - 在公有云平台上，kube-controller-manager 会自动为所有 Node 配置路由，但如果配置不当（如认证授权失败、超出配额等），也有可能导致无法配置路由

- Service NodePort 和 health probe 端口冲突

  - 在 1.10.4 版本之前的集群中，多个不同的 Service 之间的 NodePort 和 health probe 端口有可能会有重合 （已经在 [kubernetes#64468](https://github.com/kubernetes/kubernetes/pull/64468) 修复）

- 主机内或者云平台的安全组、防火墙或者安全策略等阻止了 Pod 网络，比如

  - 非 Kubernetes 管理的 iptables 规则禁止了 Pod 网络

  - 公有云平台的安全组禁止了 Pod 网络（注意 Pod 网络有可能与 Node 网络不在同一个网段）

  - 交换机或者路由器的 ACL 禁止了 Pod 网络

    

  1、进入pod内互相发包，使用tcpdump抓包查看数据发送情况

  2、网络插件出现问题（flannel、calico）

  **其他网络问题请查看**： [网络异常排错](https://feisky.gitbooks.io/kubernetes/content/troubleshooting/network.html)

### 7、kube-proxy挂掉的影响

无法通过service访问到pod，业务将直接出于瘫痪状态

### 8、k8s pod一直处于pending状态、一直处于 Waiting 或 ContainerCreating 状态

1、资源不足，无法进行调度（pending）

2、镜像拉取失败

3、CNI 网络错误，一般需要检查 CNI 网络插件的配置

4、镜像打包是否正确或者是否配置了正确的参数

**其他pod问题请查看：[Pod 异常排错](https://feisky.gitbooks.io/kubernetes/content/troubleshooting/pod.html)**

### 9、redis在内存大（比如内存32G）的情况下应该注意哪些问题

1、主库宕机，因数据太大，导致从库恢复时间拉长

2、扩容时间拉长

3、如果网络不良，将导致从库重做，引发雪崩

4、 内存越大，触发持久化的操作阻塞主线程的时间越长

**解决办法：**

解决办法当然就是极力减少内存的使用了，一般情况下，我们都是这么做的：

**1 设置过期时间**

对具有时效性的key设置过期时间，通过redis自身的过期key清理策略来降低过期key对于内存的占用，同时也能够减少业务的麻烦，不需要定期清理了

**2 不存放垃圾到redis中**

这简直就是废话，但是，有跟我们同病相怜的人么?

**3 及时清理无用数据**

例如一个redis承载了3个业务的数据，一段时间后有2个业务下线了，那你就把这两个业务的相关数据清理了呗

**4 尽量对数据进行压缩**

例如一些长文本形式的数据，压缩能够大幅度降低内存占用

**5 关注内存增长并定位大容量key**

不管是DBA还是开发人员，你用redis，你就必须关注内存，否则，你其实就是不称职的，这里可以分析redis实例中哪些key比较大从而帮助业务快速定位异常key(非预期增长的key，往往是问题之源)

### 10、如何处理访问量暴增 

不可预测的：网络攻击、恶意刷量

可预测的：突然爆发的社会热点，营销活动的宣传

解决方法：提前对应用进行扩容，使用CDN，减轻源站压力

### 11、数据库表太大如何处理

1、sql语句优化

- 使用limit对查询结果的记录进行限定
- 避免select *，将需要查找的字段列出来
- 少用JOIN

2、分表：水平分表、垂直分表

### 12、mysql主从同步延迟

最简单的减少slave同步延时的方案就是在架构上做优化，尽量让主库的DDL快速执行。还有就是主库是写，对数据安全性较高，比如 sync_binlog=1，innodb_flush_log_at_trx_commit = 1 之类的设置，而slave则不需要这么高的数据安全，完全可以讲sync_binlog设置为0或者关闭binlog，innodb_flushlog也 可以设置为0来提高sql的执行效率。另外就是使用比主库更好的硬件设备作为slave。

### 13、锁冲突与死锁问题

- 尽量使用较低的隔离级别； 精心设计索引，并尽量使用索引访问数据，使加锁更精确，从而减少锁冲突的机会；
- 选择合理的事务大小，小事务发生锁冲突的几率也更小；
- 给记录集显式加锁时，最好一次性请求足够级别的锁。比如要修改数据的话，最好直接申请排他锁，而不是先申请共享锁，修改时再请求排他锁，这样容易产生死锁；
- 不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行。这样可以大大减少死锁的机会；
- 尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响； 不要申请超过实际需要的锁级别；除非必须，查询时不要显示加锁；
- 对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能。

